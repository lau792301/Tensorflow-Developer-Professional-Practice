Question 61
point
6. Question 6
When predicting words to generate poetry, the more words predicted the more likely it will end up gibberish. Why?


It doesn�t, the likelihood of gibberish doesn�t change


Because you are more likely to hit words not in the training set


Because the probability of prediction compounds, and thus increases overall


- Because the probability that each word matches an existing phrase goes down the more words you create

Question 71
point
7. Question 7
What is a major drawback of word-based training for text generation instead of character-based generation?


There is no major drawback, it�s always better to do word-based training


Character based generation is more accurate because there are less characters to predict


Word based generation is more accurate because there is a larger body of words to draw from


- Because there are far more words in a typical corpus than characters, it is much more memory intensive

Question 81
point
8. Question 8
How does an LSTM help understand meaning when words that qualify each other aren�t necessarily beside each other in a sentence?


- Values from earlier words can be carried to later ones via a cell state


They don�t


They shuffle the words randomly


They load all words into a cell state


Question 3
What are the critical steps in preparing the input sequences for the prediction model?


- Pre-padding the subprhases sequences.


Splitting the dataset into training and testing sentences.


- Generating subphrases from each line using n_gram_sequences. 


Converting the seed text to a token sequence using texts_to_sequences.

Question 4
In  natural language processing, predicting the next item in a sequence is a classification problem.Therefore, after creating inputs and labels from the subphrases, we one-hot encode the labels.
What function do we use to create one-hot encoded arrays of the labels?


tf.keras.utils.img_to_array


- tf.keras.utils.to_categorical


tf.keras.preprocessing.text.one_hot


tf.keras.utils.SequenceEnqueuer

5.
Question 5
True or False: When building the model, we use a sigmoid activated Dense output layer with one neuron per word that lights up when we predict a given word.

- False

True